# MLP from Scratch
### I made this project in order to get a better understanding in Deep Learning. 
<br> 

## About

- The model is coded using _Numpy_, _MpMath_, and _Matplotlib_ libraries
- As task I choosed "**XOR Gate Learning**", which is one of the basis task for deep learning models 
- The activation function is the "**Sigmoid**" function for all layers
- The model contains:
  - 1 input layer with 2 neurons
  - 1 hidden layer with 4 neurons
  - 1 output layer with 1 neuron
- Weights are initiliazed respect to "**Normal Distribution**"
<br>

## Results

- Sample number: **80** (4*20)
- Training duration: **28 mins 15 secs** 
- Epoch number: **5000**
- Learning rate: **0.003**
- Loss: **0.0002100859**
- Accuracy: **% 97.87174999**
- Loss / Accuracy Graph: <br>
![This is an image](https://github.com/IsmailKonak/MLP-From-Scratch/blob/main/Loss_Accuracy.png)
<br>
<br>
## Reference

I have benefited greatly from the following resources in this process:
- [Medium](https://medium.com/@tiago.tmleite/neural-networks-multilayer-perceptron-and-the-backpropagation-algorithm-a5cd5b904fde) - Neural Networks, Multilayer Perceptron and the Backpropagation Algorithm by _Tiago M. Leite_

- [Youtube](https://www.youtube.com/watch?v=tIeHLnjs5U8) - Backpropagation calculus | Chapter 4, Deep learning

- [Youtube](https://www.youtube.com/watch?v=sDv4f4s2SB8) - Gradient Descent, Step-by-Step

- [Youtube](https://www.youtube.com/watch?v=IHZwWFHWa-w) - Gradient descent, how neural networks learn | Chapter 2, Deep learning
- Book - Deep Learning, **_Ian Goodfellow_** and **_Yoshua Bengio_** and **_Aaron Courville_**
